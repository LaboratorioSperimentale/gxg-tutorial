{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2a0ad3-e008-42a8-bbcb-ac72bd28ae3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Italian Computational Linguistics: an example case from EVALITA\n",
    "\n",
    "Linguistica Applicata (dott.ssa Silvia Ballarè)\n",
    "\n",
    "6 dicembre 2023\n",
    "\n",
    "Ludovica Pannitto - Laboratorio Sperimentale LILEC\n",
    "\n",
    "ludovica.pannitto@unibo.it\n",
    "\n",
    "lilec.lab@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951fe95-0c3a-4197-ba2f-f12f6ef88dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Laboratorio](../imgs/lab.png \"Sito Lab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6839215-2917-4c43-8297-13351b0786e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Laboratorio](../imgs/mission.png \"Mission Lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2fbcb-64ad-449b-b773-f5b0c6f82751",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Aim of this tutorial:\n",
    "\n",
    "* Get to know the Italian Computational Linguistics (CL) community and their initiatives\n",
    "* Familiarize with the process of building a CL model\n",
    "* See some example snippets of code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1825c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Lecture outline:\n",
    "\n",
    "1. Present the EVALITA campaign\n",
    "2. Explore GxG task (from EVALITA 2018)\n",
    "3. Load and manipulate data\n",
    "4. Outline the main steps to reproduce one of the system that participated in the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c5206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# EVALITA\n",
    "\n",
    "\n",
    "[Evalita](https://www.evalita.it/) is a periodic evaluation campaign of Natural Language Processing (NLP) and speech tools for the Italian language.\n",
    "\n",
    "EVALITA is an initiative of the [Italian Association for Computational Linguistics (AILC)](https://www.ai-lc.it/en/) and it is endorsed by the [Italian Association for Artificial Intelligence (AI\\*IA)](https://aixia.it/en/) and the [Italian Association for Speech Sciences (AISV)](https://www.aisv.it/).\n",
    "\n",
    "\n",
    "![EVALITA](../imgs/EVALITA.png \"EVALITA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaae1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## What does it mean?\n",
    "\n",
    "Every two years (approx.), the community decides on some _tasks_ that are considered to be interesting or challenging for some reasons.\n",
    "\n",
    "Then researchers develop models to tackle the poposed _shared task_ and compete. A final analysis that highlights the strenghts of the different submitted models hopefully allows for a broader discussion on the task itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167290c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Why is it important?\n",
    "\n",
    "The general objective of EVALITA is to **promote the development of language and speech technologies** for the Italian language, providing a **shared framework** where different systems and approaches can be evaluated in a consistent manner.\n",
    "\n",
    "\n",
    "The diffusion of shared tasks and shared evaluation practices is a crucial step towards the development of resources and tools for NLP and speech sciences.\n",
    "\n",
    "\n",
    "As a side effect of the evaluation campaign, both training and test data are available to the scientific community as benchmarks for future improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2613723",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Example tasks from past years:\n",
    "\n",
    "| year | tasks list                         |\n",
    "| ---  | ---                                |\n",
    "| 2007 | Part of Speech Tagging             |\n",
    "|      | Named Entity Recognition           |\n",
    "| 2011 | Anaphora Resolution                |\n",
    "|      | Frame Labeling over Italian Texts  |\n",
    "| 2014 | Sentiment Polarity Classification  |\n",
    "|      | Dependency Parsing                 |\n",
    "| 2020 | Stance Detection                   |\n",
    "|      | Multimodal Artefacts Recognition   |\n",
    "|      | Ghigliottin-AI                     |\n",
    "\n",
    "\n",
    "... to be continued!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773884f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# GxG (from Evalita 2018)\n",
    "\n",
    "- [**Gender-X-Genre (GxG)**](https://sites.google.com/view/gxg2018) is a task on **author profiling** (in terms of gender) on Italian texts, with a specific focus on **cross-genre** performance.\n",
    "\n",
    "- The [published paper](https://ceur-ws.org/Vol-2263/paper006.pdf)[<sup>1</sup>](#fn1) presents the task of a type of author profiling task: \n",
    "> **Author profiling is the task of automatically discovering latent user attributes from text**. Gender, which we focus on in this paper, and which is traditionally characterised as a binary feature, is one of such attributes.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<sub><span id=\"fn1\">1: Dell’Orletta, Felice, and Malvina Nissim. \"Overview of the evalita 2018 cross-genre gender prediction (gxg) task.\" EVALITA Evaluation of NLP and Speech Tools for Italian 12.1 (2018): 35.</span></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f2e28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Motivation:\n",
    "\n",
    "> ...we have not yet found the actual dataset-independent features that do indeed capture the way females and males might write differently. (And might let us wonder if this is a valid assumption at all.)\n",
    "\n",
    "\n",
    "> if we can make gender prediction stable across very different genres, then we are more likely to have captured **deeper gender-specific traits** rather than dataset characteristics. As a by product, this task will yield a variety of models for gender prediction in Italian, also shedding light on **which genres favour or discourage in a way gender expression**, by looking at whether they are easier or harder to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8bb40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Computational modeling: \n",
    "\n",
    "> Given a (collection of) text(s) from a specific genre, the gender of the author has to be predicted.\n",
    "\n",
    "\n",
    "> The task is cast as a **binary classification task**, with gender represented as F (female) or M (male). \n",
    "\n",
    "\n",
    "> Evaluation settings were designed bearing in mind the question at the core of this task: are there indicative traits across genres that can be leveraged to model gender in a rather genre-independent way?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fccdb20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Which genres?\n",
    "\n",
    "Considered genres:\n",
    "- tweets\n",
    "- youtube comments (from manually selected videos from a few general topics: travel, music, documentaries, politics)\n",
    "- children writing ([essays](https://aclanthology.org/L16-1014/) written by Italian L1 learners collected during the first and second year of lower secondary school[<sup>2</sup>](#fn2))\n",
    "- journalism (single-authored newspaper article from _La Repubblica_ and _Corriere della Sera_)\n",
    "- personal diaries (freely available as part of the [_Fondazione Archivio Diaristico Nazionale della Città di Pieve Santo Stefano_](http://archiviodiari.org/index.php/iniziative-e-progetti/brani-di-dirai.html))\n",
    "\n",
    "---\n",
    "\n",
    "<sub><span id=\"fn2\">2: Alessia Barbagli, Pietro Lucisano, Felice Dell’Orletta, Simonetta Montemagni, and Giulia Venturi. 2016. Cita: an l1 italian learners corpus to study the development of writing competence. In _Proceedings of the 10 thConference on Language Resources and Evaluation (LREC 2016)_</span></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be4ff9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The competition:\n",
    "\n",
    "> In the cross-genre setting[<sup>3</sup>](#fn3), the only constraint is not using in training any single instance from the genre they are testing on. Other than that, participants were free to combine the other\n",
    "datasets as they wished.\n",
    "\n",
    "\n",
    "\n",
    "> Participants were also free to use external resources, provided the cross-genre settings were carefully preserved, and everything used was described in detail in their final report.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<sub><span id=\"fn3\">3: participants were also encouraged to submit a same-genre model.</span></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a96b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Measures:\n",
    "\n",
    "> average **accuracy** for the two classes, i.e. F and M. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8da3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Baselines:\n",
    "\n",
    "\n",
    "> For all settings, given that the datasets are balanced for gender distribution, through random assignment we will have 50% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095eb3bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# (Partial) Results\n",
    "\n",
    "(Partial table of) results in terms of Accuracy of the Cross-Genre task\n",
    "\n",
    "\n",
    "| Model Name           | CH    | DI    | JO    | TW    | YT    |\n",
    "| ----------           | ---   | ---   | ---   | ---   | ---   |\n",
    "| CapetownMilanoTirana | 0.535 | 0.635 | 0.515 | 0.555 | 0.503 |\n",
    "| ItaliaNLP - SVM      | 0.540 | 0.514 | 0.505 | 0.586 | 0.513 |\n",
    "| ItaliaNLP - STL      | 0.640 | 0.554 | 0.495 | 0.609 | 0.510 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ec512",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Participants \n",
    "\n",
    "> **[CapetownMilanoTirana](https://ceur-ws.org/Vol-2263/paper028.pdf)**[<sup>4</sup>](#fn4):  classifier based on Support Vector Machine (SVM) as learning algorithm. They tested different n-gram features extracted at the word level as well as at the character level. In addition, they experimented feature abstraction transforming each word into a list of symbols and computing the length of the obtained word and its frequency.\n",
    "\n",
    "\n",
    "> **[ItaliaNLP](https://ceur-ws.org/Vol-2263/paper013.pdf)**[<sup>5</sup>](#fn5) tested three different classification models: one based on linear SVM, and two based on Bi-directional Long Short Term Memory (Bi-LSTM). The two deep neural network architectures use 2-layers of Bi-LSTM. The first Bi-LSTM layer encodes each sentence as a token sequence, the second layer encodes the sentence sequence.\n",
    "\n",
    "---\n",
    "\n",
    "<sub><span id=\"fn4\">4: Angelo Basile, Gareth Dwyer, and Chiara Rubagotti. 2018. _CapetownMilanoTirana_ for GxG at Evalita2018. Simple n-gram based models perform well for gender prediction. Sometimes. In Tommaso Caselli, Nicole Novielli, Viviana Patti, and Paolo Rosso, editors, _Proceedings of Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2018)_, Turin, Italy. \n",
    "\n",
    "<sub><span id=\"fn5\">5: Andrea Cimino, Lorenzo De Mattei, and Felice Dell’Orletta. 2018. Multi-task Learning in Deep Neural Networks at EVALITA 2018. In Tommaso Caselli, Nicole Novielli, Viviana Patti, and Paolo Rosso, editors, _Proceedings of Sixth Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA\n",
    "2018)_, Turin, Italy</span>\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397f748",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data\n",
    "\n",
    "Organizers provided training data in pseudo-xml format as follows:\n",
    "\n",
    "\n",
    "| Dataset   | F    | M     | Tokens |\n",
    "| ---       | ---  | ---   | ---    |\n",
    "| Children  | 100  | 100   | 65986  |\n",
    "| Diaries   | 100  | 100   | 82989  |\n",
    "|Journalism | 100  | 100   | 113437 |\n",
    "| Twitter   | 3000 | 3000  | 101534 |\n",
    "| Youtube   | 2200 | 2200  | 90639  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43c246",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Examples from data:\n",
    "\n",
    "```\n",
    "<doc id=\"3140\" genre=\"youtube\" gender=\"M\">\n",
    "Ti sei spiegata benissimo complimenti\n",
    "</doc>\n",
    "<doc id=\"5744\" genre=\"youtube\" gender=\"M\">\n",
    "Salvini e' veramente uno schifoso. Nonostante tutto, un sacco di POLLI lo hanno votato.\n",
    "</doc>\n",
    "<doc id=\"9766\" genre=\"youtube\" gender=\"F\">\n",
    "favi fai un video dove assaggiate cibi strani del Giappone! dai zio che esce una figata pazzesca! spolliciate per farglielo leggere\n",
    "</doc>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9eb05-0126-41f1-9fb8-2e5e6857c654",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Training - Test - Gold\n",
    "\n",
    "Along with the training set (shown in previous slide), organizers also provide:\n",
    "\n",
    "* Test set: same as training, but the attribute `gender` is unknown  \n",
    "  ```\n",
    "  <doc id=\"27618\" genre=\"twitter\" gender=\"?\">\n",
    "    @zioburp pregava gli scossoni del treno non le facessero scappare le maglie.\n",
    "  </doc>\n",
    "  ```\n",
    "\n",
    "\n",
    "* Gold labels: the files, released after the competition, contain the _solution_ to test set items\n",
    "    ```\n",
    "    2757\tM\n",
    "    2758\tM\n",
    "    2759\tF\n",
    "    2760\tM\n",
    "    2761\tM\n",
    "    2762\tM\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c644e8b-291e-45be-94ff-208f0090ca03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The pipeline\n",
    "\n",
    "Both the participating models use pretty much the same approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c7de7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Each document is represented as a vector (list of features). Its label (M or F) is represented as a binary value (i.e., 0 for M and 1 for F)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d08e49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "> A classifier (see Support Vector Machine slide) is trained with cross-validation on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a8abe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "> Test documents are then represented with the same set of features and the model is used to assign a class (0 or 1) to each document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf55a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Accuracy is calculated on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd51f5-25d8-47b0-b366-7e7b25f48cc2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The classifier: Support Vector Machine\n",
    "\n",
    "![Support Vector Machine](../imgs/SVM.png \"Support Vector Machine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4612ad1-efc2-494f-9af9-82b25de6fe49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Steps:\n",
    "\n",
    "1. Process data\n",
    "2. Represent data in a suitable format\n",
    "3. Build classifier\n",
    "4. Classify new (`test`) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fd821-e66a-4b04-bacc-59207e3116a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step 1: Data Pre-processing\n",
    "\n",
    "Before performing any linguistic annotation, data needs to be checked and _cleaned_. More specifically, concerning our data, we see:\n",
    "\n",
    "- mentions\n",
    "- links + emails\n",
    "- hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7e40e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## How?\n",
    "A common method is employing **regular expressions**: we need to look at training data and come up with some reasonable transformation. We then will apply the same transformation to test data.\n",
    "\n",
    "\n",
    "We could do more: lowercase words that are entirely upper-cased, remove emoticons, normalize cases of letter repetitions etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e82d9-52f5-4461-8a69-5348cf58f0e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prelimiar step (1): mentions `@'\n",
    "\n",
    "`Mi è piaciuto un video di @YouTube da @carodexter`\n",
    "\n",
    "`@disinformatico comincia a sembrare Internet delle Co@@ionate... :-D`\n",
    "\n",
    "`@Reggiace neppure ad essi era concesso sbirciare le grazie di Sua Maestà Carolina d'Asburgo @CasertaReggia @CarolCarditello`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa3ce8-3abb-448a-b75a-76248e78513f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prelimiar step (2): links and emails\n",
    "\n",
    "`Ahahah grande lassss — Essì https:// l.ask.fm/igoto/45DKECPW 7B667HQMHN2IG6NM56EDDIA3SGTODHVVBSS2J7FI2AT5AJKQFZEM6QRTKG3PYFAEIFXTKFAU34N7A55YPPSKNTBAUYOADFKYCASCGMX5GWID6KPLQSVKBSDZSKQY2H2QZTGX7K4GJSNBWTLK67BSL4ET2LHV77K3QK236UUDLFOUVQHX5FR2P3XGTIUKEOQF7U====== …`\n",
    "\n",
    "`Riassumila facendo riferimento a questo documento che è talmente autorevole da essere stato censurato dalla tv italiana: https://www.youtube.com/watch?v=6ZUdDj8rv4E`\n",
    "\n",
    "`più info sul sito ufficiale www.newbikeproducts.com/it`\n",
    "\n",
    "`ciao Gabri,ho visto il tuo filmato torino islanda,sono anch' io una ciclista di 68 anni,certo non faccio quello che fai tu mi sono divertita molto vederdi\n",
    "ciao.lucia.foieni@gmail.com`\n",
    "\n",
    "`Qui le Nostre Offerte di lavoro. diego.lugato@swegon.it TRE INTERESSANTI POSIZIONI APERTE https:// lnkd.in/datkNAk`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276d7d1-d10c-49f1-89f6-aa2146bc262b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prelimiar step (3): hashtags\n",
    "\n",
    "`Chissà come farà #Salvini a stare nella solita coalizione con chi era al governo insieme alla fornero. #Passera `\n",
    "\n",
    "`@alessiarotta tu non sei degna nemmeno di nominarlo buffona #mangiapaneatradimento `\n",
    "\n",
    "`#heartbeatoftheday cosa succederebbe se invece di boss avessimo coach? #leadership #lovemarketing #coaching pic.twitter.com/MLE3R2syep `     \n",
    "\n",
    "`#Ventura :\"Ho escluso #Baselli e #Zappacosta perché devono capire quale strada seguire per diventare protagonisti in SerieA\". Dalla panchina? `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e740459-5f6b-4b33-9614-5bcdb694485e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Proposal (from easier to harder) - Hashtags:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842f511",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- replace with generic `_HASHTAG_` token\n",
    "- some of them are useful for understanding the sentence, so we could just remove the `#` symbol\n",
    "- could we keep track of how many substitutions we're performing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a15837-fbc8-441f-ac47-1b0f818a2bf2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### `#([^ ]+)` -> `$1 `\n",
    "\n",
    "- `#` matches the character # with index 3510 (2316 or 438) literally (case sensitive)\n",
    "- 1st Capturing Group `([^ ]+)`\n",
    "    - Match a single character not present in the list below `[^ ]`\n",
    "    - `+` matches the previous token between one and unlimited times, as many times as possible\n",
    "    - ` ` matches the character [space]\n",
    "-  matches the character [space]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6386bb-015e-4d06-ad5d-fb7cbd551eba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Proposal (from easier to harder) - Mentions:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66666a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- every @something is transformed into `user` \n",
    "- some represent known cases (i.e., `@YouTube`, mentions to newspapers etc). Do we want to keep that?\n",
    "- can we differentiate based on the context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c345fc-eb79-4b26-9659-093658562767",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### `@YouTube` -> `YouTube`, `@[^ ]+ ` -> `user `\n",
    "\n",
    "\n",
    "- `@` matches the character `@` \n",
    "- Match a single character not present in the list below `[^ ]`\n",
    "    - `+` matches the previous token between one and unlimited times, as many times as possible\n",
    "    - ` ` matches the character [space]\n",
    "- ` ` matches the character [space]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69877a0-125e-4507-8bfc-cd853a79ce7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Proposal (from easier to harder) - Links and emails:        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bbcf3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- emails are easily searcheable and can be transformed into `this address` for instance\n",
    "- links are hard to find: try our best with a complex regular expression and substitute them with `link`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf56a6-c98f-4d44-b4e3-1037c2e6fa46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### `\\b[A-z0-9\\.]+@[A-z0-9\\.]+\\.[a-z]+\\b` -> `this address`\n",
    "\n",
    "- `\\b` assert position at a word boundary\n",
    "- Match a single character present in the list below `[A-z0-9\\.]`\n",
    "    - `+` matches the previous token between one and unlimited times, as many times as possible\n",
    "    - `A-z` matches a single character in the range between A and z\n",
    "    - `0-9` matches a single character in the range between 0 and 9\n",
    "    - `\\.` matches the character `.` \n",
    "- `@` matches the character `@` \n",
    "- Match a single character present in the list below `[A-z0-9\\.]`\n",
    "    - `+` matches the previous token between one and unlimited times, as many times as possible\n",
    "    - `A-z` matches a single character in the range between A and z\n",
    "    - `0-9` matches a single character in the range between 0 and 9\n",
    "    - `\\.` matches the character `.` \n",
    "- `\\.` matches the character `.`\n",
    "- Match a single character present in the list below `[a-z]`\n",
    "    - `+` matches the previous token between one and unlimited times, as many times as possible\n",
    "    - `a-z` matches a single character in the range between a and z \n",
    "- `\\b` assert position at a word boundary\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf8a1c-c45d-4be1-a89f-75a66339c460",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### `\\b([A-z0-9]+[\\/\\.][A-z0-9]+[\\/\\.]?)+[A-z0-9]*\\b` -> `link`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f9a74-5aab-4418-bcf7-605940612cb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step 2: Linguistic Pipeline\n",
    "\n",
    "The text provided by organizers is raw. If we want to extract linguistic information from it we might need some annotation on top of raw text.\n",
    "This can be done by hand by manual annotators, or with automatic tools.\n",
    "\n",
    "We will take a look at some automatic tools: specifically [`spacy`](https://spacy.io/), a python library for natural language processing.\n",
    "\n",
    "`spaCy` also provides a model for italian trained on news and media written text.\n",
    "\n",
    "\n",
    "But **first things first**: anyone familiar with Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9050f-3c4c-4800-84f6-5f80d14efa71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python 101\n",
    "\n",
    "## There are tree fundamental types of entities: number, strings and booleans\n",
    "\n",
    "print(3+7)\n",
    "\n",
    "print(\"Hello world\")\n",
    "\n",
    "print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216afb4a-7e4a-4622-bfe6-852c32a9c7b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We can assign values to variables\n",
    "\n",
    "n = 13\n",
    "name = \"Ludovica\"\n",
    "is_italian = True\n",
    "\n",
    "## and perform operations on them\n",
    "\n",
    "print(n+3)\n",
    "print(name[0]+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529161e-4c39-4790-8074-5117d6973450",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Things can get more complicated\n",
    "\n",
    "if is_italian:\n",
    "    print(\"Ciao\", name, \", a casa tutto bene?\")\n",
    "else:\n",
    "    print(\"Hey there\", name, \", where are you from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ca4ab-f917-42c1-a716-d3b2302f3220",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Using `spaCy`\n",
    "\n",
    "Let's turn to linguistic processing now.\n",
    "\n",
    "Let's see how to do:\n",
    "1. load a model in spacy\n",
    "2. process text\n",
    "   - split text into sentences\n",
    "   - lemma and Part-of-Speech tagging\n",
    "   - morphological analysis\n",
    "   - dependency parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec0870",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Sei anni dopo una riforma che fu definita epocale, \"\\\n",
    "        \"la scuola superiore cambia volto. Crolla il liceo \"\\\n",
    "        \"classico e cambia pelle lo scientifico, che diventa sempre più light.\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a124e40-1b95-4173-b665-11d0410f587b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SpaCy and parse text\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp_pipeline = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "parsed_text = nlp_pipeline(text)\n",
    "\n",
    "print(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f27bb9-df26-4c3b-87e7-4c2a25595c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print sentences one by one\n",
    "\n",
    "sentences = list(parsed_text.sents)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"SENTENCE:\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6039ffb-dc64-42b6-a315-81a7c63bd46f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print tokens\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"SENTENCE N.\", i)\n",
    "    i = i+1\n",
    "    \n",
    "    for token in sentence:\n",
    "        print(token,\"\\t\", token.lemma_,\"\\t\", token.pos_)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd357c3e-3197-4632-91d0-caaf582a9918",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print morphological analysis\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"SENTENCE N.\", i)\n",
    "    i = i+1\n",
    "    \n",
    "    for token in sentence:\n",
    "        print(token,\"\\t\", token.lemma_,\"\\t\", token.morph)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e769a22-bd88-4ed5-a0ee-5d51672bbe65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print dependency parsing\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"SENTENCE N.\", i)\n",
    "    i = i+1\n",
    "    \n",
    "    for token in sentence:\n",
    "        print(token.i, \"\\t\", token,\"\\t\", token.head.i,\"\\t\", token.dep_)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0ce07-8746-49ca-923b-cfa57ed1b9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save syntactic trees to a file\n",
    "\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "\n",
    "#svg = displacy.render(sentences, style=\"dep\", jupyter=False)\n",
    "for i, sent in enumerate(sentences):\n",
    "    svg = displacy.render(sent, style=\"dep\", jupyter=False)\n",
    "    \n",
    "    filepath = Path(f\"dependency_plots_{i}.svg\")\n",
    "    filepath.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61129215-d519-4dc8-9711-270818dfe294",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step 3: Represent Documents\n",
    "\n",
    "We now have both training and test data in a new format:\n",
    "\n",
    "```\n",
    "### 1\tjournalism\tM\n",
    "0\tE\te\tCCONJ\t\tcc\t4\n",
    "1\ti\til\tDET\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\tdet\t2\n",
    "2\tgiovani\tgiovane\tNOUN\tNumber=Plur\tnsubj\t4\n",
    "3\titaliani\titaliano\tADJ\tGender=Masc|Number=Plur\tamod\t2\n",
    "4\tvivono\tvivere\tVERB\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\tROOT\t4\n",
    "5\tancora\tancora\tADV\t\tadvmod\t4\n",
    "6\tcon\tcon\tADP\t\tcase\t8\n",
    "7\ti\til\tDET\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\tdet\t8\n",
    "8\tgenitori\tgenitore\tNOUN\tGender=Masc|Number=Plur\tobl\t4\n",
    "9\t.\t.\tPUNCT\t\tpunct\t4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e775a-1475-4d72-b015-f52244800897",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Types of features\n",
    "\n",
    "- **ItaliaNLP model**:\n",
    "    * Raw and Lexical Text Features <sub>(number of tokens, character n-grams, word n-grams, lemma n-grams, repetition of n-grams chars, number of mentions, number of hashtags, punctuation.)</sub>\n",
    "    * Morpho-syntactic Features <sub>(coarse grained Part-Of-Speech n-grams, Fine grained Part-Of-Speech n-grams, Coarse grained Part-Of-Speech distribution)</sub>\n",
    "    * Lexicon features <sub>(Emoticons Presence, Lemma sentiment polarity n-grams, Polarity modifier, PMI score, sentiment polarity distribution, Most frequent sentiment polarity, Sentiment polarity in text sections, Word embeddings combination.)</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db263a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **CapetownMilanoTirana model**:\n",
    "    * n-grams extracted at the word level as well as at the character level (3-10 n-grams and binary TF-IDF)\n",
    "    * experiment with feature abstraction following the [bleaching approach](https://arxiv.org/pdf/1805.03122.pdf)[<sup>6</sup>](#fn6)\n",
    " \n",
    "---\n",
    "<sub><span id=\"fn6\">6: Rob van der Goot, Nikola Ljubesic, Ian Matroos, Malvina Nissim, and Barbara Plank. 2018. Bleaching text: Abstract features for cross-lingual gender prediction. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_</span>\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938728ed-7219-44a5-b94d-957790a4a334",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Bleaching process\n",
    "\n",
    "|   | SHAPE | FREQ | LEN | ALPHA | \n",
    "|---|---    | ---  | --- | ---   |\n",
    "| Questo  | Cvvccv | 46 | 06 | True |\n",
    "| è | v | 650 | 01 | True |\n",
    "| solo | cvcv | 116 | 04 | True |\n",
    "| un | vc | 1 | 02 | True |\n",
    "| esempio | vcvccvv | 1 | 07 | True |\n",
    "| . | . | 60 | 01 | False |\n",
    "| 😃 | 😃 | 0 | 1 | False |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2784c7f-5439-4808-b9b3-321df9f164cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Different features:\n",
    "- some are just \"scores\" assigned to each document (i.e. `number of tokens`, `emoticons presence`)\n",
    "  * `Questo è solo un esempio . 😃` -> `7`\n",
    "- others transform the document in a series of elements (i.e. `freq`, `alpha`)\n",
    "  * `Questo è solo un esempio . 😃` -> `[1,1,1,1,1,0,0]`\n",
    "- others need a \"vocabulary\" (i.e., `ngrams`)\n",
    "  * `Questo è solo un esempio . 😃` -> `[1, 1, 0, 0, 0, 1, 1, 1, 0, ...]` where positions in this list correspond to the presence of a specific n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0a8c5-1f95-43f3-818c-dca3ed5189fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "with open(\"example_tweet.conll\") as fin:\n",
    "    fin.readline()\n",
    "    for line in fin:\n",
    "        if len(line)>1:\n",
    "            text.append(line.strip().split(\"\\t\"))\n",
    "\n",
    "for line in text:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fae92e-c64b-463f-9b2c-510dcebcd1a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## number of tokens\n",
    "number_of_tokens = len(text)\n",
    "\n",
    "## emoticons presence\n",
    "emoticons_list = [\":-)\", \":)\", \":(\", \":D\"]\n",
    "emoticons_presence = False\n",
    "for token in text:\n",
    "    lemma = token[2]\n",
    "    if lemma in emoticons_list:\n",
    "        emoticons_presence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce60ec-8439-4e32-bbd9-0ef47a1d42dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Alpha\n",
    "\n",
    "repr_alpha = []\n",
    "for token in text:\n",
    "    lemma = token[2]\n",
    "    if all (c.isalpha() for c in lemma):\n",
    "        repr_alpha.append(1)\n",
    "    else:\n",
    "        repr_alpha.append(0)\n",
    "\n",
    "print(repr_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569f49c-5506-49de-b206-120440b6f274",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute vocabulary and token frequencies\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "with open(\"TransformedData/training_parsed.txt\") as fin:\n",
    "    \n",
    "    for line in fin:\n",
    "        if not line.startswith(\"###\"):\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line)>1:\n",
    "                lemma = line[2]\n",
    "                if not lemma in vocabulary:\n",
    "                    vocabulary[lemma] = 0\n",
    "                else:\n",
    "                    vocabulary[lemma] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1bdcb-a2b0-4619-be12-90565108caa0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_vocabulary = sorted(vocabulary.items(), key = lambda x: -x[1])\n",
    "\n",
    "print(sorted_vocabulary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7a178-219b-4c09-8631-33f0bf9b0123",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frequencies\n",
    "\n",
    "frequencies_list = []\n",
    "for token in text:\n",
    "    lemma = token[2]\n",
    "    frequencies_list.append(vocabulary[lemma])\n",
    "\n",
    "print(frequencies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab711f4-51b2-424a-81c9-f4a390cb9b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "\n",
    "vocabulary_list = dict(zip(vocabulary.keys(), range(len(vocabulary))))\n",
    "\n",
    "bag_of_words = []\n",
    "for token in text:\n",
    "    lemma = token[2]\n",
    "    if lemma in vocabulary_list:\n",
    "        bag_of_words.append(vocabulary_list[lemma])\n",
    "\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3842f6-4e93-4ebb-8332-5ce03803e354",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step 4: Build Classifier\n",
    "\n",
    "Building a (basic) Support Vector Machine Classifier is extremely easy in Python.\n",
    "\n",
    "\n",
    "Let's take a quick look at the [`scikit-learn`](https://scikit-learn.org) library and the [SVM](https://scikit-learn.org/stable/modules/svm.html) function implemented there.\n",
    "\n",
    "All we need is a set of training data, associated with a set of `0/1` labels representing their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef249-6d02-4946-9012-5d35dbf9f2ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "training_data = [[0, 0], [1, 1]]\n",
    "labels = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(training_data, labels)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a1f46-54d6-4ca6-95a9-576c1165178b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_datapoint = [2, 4]\n",
    "\n",
    "print(clf.predict([new_datapoint]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc0f4d-abec-4e79-a5cc-50bafcab15cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_datapoint = [-2, -1]\n",
    "print(clf.predict([new_datapoint]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf7c03-1625-4097-98ad-d24431a554fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# we create 40 separable points\n",
    "X, y = make_blobs(n_samples=10, centers=2, random_state=6)\n",
    "\n",
    "i=0\n",
    "while i<len(X):\n",
    "    print(\"POINT:\", X[i])\n",
    "    print(\"Label:\", y[i])\n",
    "    print()\n",
    "    i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe6808-43d8-4270-895f-a1e8c0b4e471",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11649e-d36b-4534-89a0-1222bb836829",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5cafa-315b-4a31-b4de-cc19c88d04ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    plot_method=\"contour\",\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    "    ax=ax,\n",
    ")\n",
    "# plot support vectors\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
